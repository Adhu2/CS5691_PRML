{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5b7709",
   "metadata": {},
   "source": [
    "# PRML ASSIGNMENT 3: SPAM/HAM\n",
    "#(Request before running code:test folder is kept empty as no test datafile is given. Please add files to this folder before running below code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b8494",
   "metadata": {},
   "source": [
    "Training Dataset spam_ham_dataset downloaded from\n",
    "https://www.kaggle.com/datasets/venky73/spam-mails-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f906e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os #to list all files in test directory\n",
    "\n",
    "#for preprocessing,vocabBuild\n",
    "from collections import UserList\n",
    "from fileinput import filename\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "\n",
    "#for vectorizer\n",
    "import ast\n",
    "\n",
    "#for naive Bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f26a9",
   "metadata": {},
   "source": [
    "Preprocess test/training data:\n",
    "Remove duplicate data, Convert data to lower case, Remove special characters and stopwords(eg:the, it..),stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "85cfa3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForPreprocessing:\n",
    "\n",
    "    def __init__(self, trainDataset, training):\n",
    "        self.training = training\n",
    "        self.trainDataset = trainDataset\n",
    "        self.processedTrainDataset = None\n",
    "\n",
    "    # remove duplicate values and convert text to lower case\n",
    "    def removeDup_makeLower(self):\n",
    "        if(self.training):\n",
    "            temp = pd.read_csv(self.trainDataset , usecols=['text', 'spam'])\n",
    "            temp.rename(columns={'spam':'label_num'}, inplace=True)\n",
    "            temp['class'] = np.where(temp['label_num'] == 1, 'spam', 'ham')\n",
    "        else:\n",
    "            temp = pd.read_csv(self.trainDataset, usecols=['text'])\n",
    "        temp.drop_duplicates(inplace=True)\n",
    "        self.processedTrainDataset = temp\n",
    "        self.processedTrainDataset['text'] = self.processedTrainDataset['text'].apply(lambda x : x.lower()) #text to lowercase\n",
    "    \n",
    "    # remove special char and stopwords\n",
    "    def removeSpCharStopword(self):\n",
    "        spChar = \"!#$%&'()*+,-./:;<=>?@[\\]\\\\^_`{|}~'\"\n",
    "        num = '0-9'\n",
    "        self.processedTrainDataset['text'] = self.processedTrainDataset['text'].apply(lambda x : re.sub(r'[{0}]'.format(spChar + '\"') , '', x))\n",
    "        self.processedTrainDataset['text'] = self.processedTrainDataset['text'].apply(lambda x : re.sub(r'[{0}]'.format(num), '', x))\n",
    "        self.processedTrainDataset[\"text\"] = self.processedTrainDataset[\"text\"].apply(lambda x: re.sub(' +', ' ', x))\n",
    "    \n",
    "        sw = set(stopwords.words('english'))\n",
    "        sw.add('http')\n",
    "        sw.add('subject')\n",
    "        self.processedTrainDataset['text'] = self.processedTrainDataset['text'].apply(lambda x : \" \".join([word for word in str(x).split() if word not in sw]))\n",
    "    \n",
    "    # reducing the words to its base form using stemming or lemmatization. Inflection\n",
    "    def inflection(self, st_lm):\n",
    "        if(st_lm == 1):\n",
    "            ps = PorterStemmer()\n",
    "            self.processedTrainDataset['text'] = self.processedTrainDataset['text'].apply(lambda x : \" \".join([ps.stem(word) for word in x.split()]))\n",
    "        else:\n",
    "            lm = WordNetLemmatizer()\n",
    "            self.processedTrainDataset['text'] = self.processedTrainDataset['text'].apply(lambda x : \" \".join([lm.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # keeping the words present in dictionary \n",
    "    def keepDictWords(self):\n",
    "        words2 = set(words.words())\n",
    "        self.processedTrainDataset['text'] = self.processedTrainDataset['text'].apply(lambda x : \" \".join([word for word in x.split() if word in words2]))    \n",
    "\n",
    "    # saving the preprocessed data as a new file\n",
    "    def saveProcessedTrainDataset(self, fileName):\n",
    "        self.processedTrainDataset.to_csv(fileName + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93336d55",
   "metadata": {},
   "source": [
    "Build the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "19dda181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSet:\n",
    "\n",
    "    def __init__(self, TestDataPath):\n",
    "        self.TestDataPath = TestDataPath\n",
    "        self.pathToSave = 'datasets/testEmails.csv'\n",
    "    \n",
    "    # creating a csv file for the test emails in the test folder\n",
    "    def makeTestCsv(self):\n",
    "        self.emails = []\n",
    "        for count, sub_files in enumerate(os.listdir(self.TestDataPath)):\n",
    "            file_path = self.TestDataPath + '/' + sub_files\n",
    "            fp = open(file_path, 'r')\n",
    "            self.emails.append(fp.read())\n",
    "            fp.close()\n",
    "    \n",
    "    # saving the csv file\n",
    "    def saveAsCsv(self):\n",
    "        dataToSave = {'text' : self.emails}\n",
    "        df = pd.DataFrame(dataToSave)\n",
    "        df.to_csv(self.pathToSave, index=False)\n",
    "    \n",
    "    # using the preprocessor class to clean the test text data\n",
    "    def cleanAndSave(self):\n",
    "        preObj = ForPreprocessing(self.pathToSave, 0)\n",
    "        preObj.removeDup_makeLower()\n",
    "        preObj.removeSpCharStopword()\n",
    "        preObj.inflection(0)\n",
    "        preObj.keepDictWords()\n",
    "        preObj.saveProcessedTrainDataset('testEmails')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacef2de",
   "metadata": {},
   "source": [
    "Vocabulary Building: make the vocabulary for all the words in the organized training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e8d7acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildVocab:\n",
    "\n",
    "    def __init__(self, ipFilepath):\n",
    "        self.ipFilepath = ipFilepath\n",
    "        self.ipData = pd.read_csv(ipFilepath)\n",
    "        self.wordsDict = {}\n",
    "        self.invFreq = {}\n",
    "        self.words2 = set(words.words())\n",
    "    \n",
    "    # build and save as text file the word vocabulary.\n",
    "    def buildWordDict(self, filename):\n",
    "        for x in range(self.ipData.shape[0]):\n",
    "            try:\n",
    "                email = self.ipData.loc[x, 'text'].split() \n",
    "                self.addToDict(email)\n",
    "                self.invDocFreq(email)\n",
    "            except:\n",
    "                self.ipData.drop(x, inplace=True)\n",
    "                continue\n",
    "        fp1 = open(filename + '.txt', 'w')\n",
    "        fp2 = open('invertedInd.txt', 'w')\n",
    "        fp1.write(str(self.wordsDict))\n",
    "        fp2.write(str(self.invFreq))\n",
    "        fp1.close()\n",
    "        fp2.close()\n",
    "        self.ipData.to_csv(self.ipFilepath, index=False)\n",
    "    \n",
    "    # adding the words to the dictionary for each email\n",
    "    def addToDict(self, email):\n",
    "        index = len(self.wordsDict)\n",
    "        for word in email:\n",
    "            if word not in self.wordsDict:\n",
    "                self.wordsDict[word] = index\n",
    "                index += 1\n",
    "    \n",
    "    # creating the inverse document frequency table\n",
    "    def invDocFreq(self, email):\n",
    "        emailSet = set(email)\n",
    "        for words in emailSet:\n",
    "            if words not in self.invFreq:\n",
    "                self.invFreq[words] = 1\n",
    "            else:\n",
    "                self.invFreq[words] += 1 \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263a129",
   "metadata": {},
   "source": [
    "Vectorize : Extract features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04c0d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorize:\n",
    "\n",
    "    def __init__(self, dataPath, wordsDataPath, train2):\n",
    "        self.dataset = pd.read_csv(dataPath)\n",
    "        self.wordsData = ast.literal_eval(open(wordsDataPath, 'r').read())\n",
    "        self.invInd = ast.literal_eval(open('invertedInd.txt', 'r').read())\n",
    "        self.dataX = np.zeros((self.dataset.shape[0], len(self.wordsData)))\n",
    "        self.invIndTable = np.zeros((self.dataset.shape[0], len(self.wordsData)))\n",
    "        self.train2 = train2\n",
    "        if(self.train2):\n",
    "            self.dataY = np.zeros((self.dataset.shape[0]))\n",
    "\n",
    "    # creating vector representation for each email in the training or testing data\n",
    "    def vectorize(self, invInd):\n",
    "        for x in range(self.dataset.shape[0]):\n",
    "            try:\n",
    "                email = self.dataset.loc[x, 'text'].split()\n",
    "                for word in email:\n",
    "                    if word in self.wordsData:\n",
    "                        self.dataX[x, self.wordsData[word]] += 1\n",
    "                        self.invIndTable[x, self.wordsData[word]] = self.invInd[word]\n",
    "            except:\n",
    "                continue\n",
    "            E1 = np.log(self.dataset.shape[0] * np.ones(self.dataX[x , :].shape))\n",
    "            E2 = np.log(self.invIndTable[x, :], out=np.zeros_like(self.invIndTable[x, :]), where=(self.invIndTable[x, :] != 0))\n",
    "            self.invIndTable[x, :] = E1 - E2 #subtract second expression from first expression\n",
    "            if(invInd):\n",
    "                self.dataX[x, :] = self.dataX[x, :] * self.invIndTable[x, :]\n",
    "            if(self.train2):\n",
    "                self.dataY[x] = self.dataset.loc[x, 'label_num']\n",
    "    \n",
    "    # save the data for training and testing as npy binary files\n",
    "    def saveData(self):\n",
    "        if(self.train2):\n",
    "            np.save('Xtrain.npy', self.dataX)\n",
    "            np.save('Ytrain.npy', self.dataY)\n",
    "        else:\n",
    "            np.save('Xtest.npy', self.dataX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9a29a",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier with Gaussian likelihood assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "acf63d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBC:\n",
    "\n",
    "    def __init__(self, dataX, train2, nClasses=2, datay=None):\n",
    "        self.train2 = train2\n",
    "        self.X = dataX\n",
    "        self.means = {}\n",
    "        self.variances = {}\n",
    "        self.classes = nClasses\n",
    "        if(self.train2):\n",
    "            if(datay is not None):\n",
    "                self.y = datay\n",
    "                self.classes = len(np.unique(datay))\n",
    "        self.noData, self.features = dataX.shape\n",
    "        self.eta = 0.00001\n",
    "        self.priors = {} \n",
    "\n",
    "    # find and save in npy file; mean and variance parameters for two classes\n",
    "    def findParameters(self):\n",
    "        for x in range(self.classes):\n",
    "            xClassC = self.X[self.y == x]\n",
    "            self.means[str(x)] = np.mean(xClassC, axis=0)\n",
    "            self.variances[str(x)] = np.var(xClassC, axis=0)\n",
    "            self.priors[str(x)] = xClassC.shape[0] / self.noData\n",
    "        np.save('means.npy', self.means)\n",
    "        np.save('variances.npy', self.variances)\n",
    "    \n",
    "    # load npy files for the parameters to classify test data\n",
    "    def loadParameters(self):\n",
    "        means = np.load('means.npy', allow_pickle=True)\n",
    "        variances = np.load('variances.npy', allow_pickle=True)\n",
    "        for x in range(self.classes):\n",
    "            self.priors[str(x)] = 1 / self.noData\n",
    "            self.means[str(x)] = means[()][str(x)]\n",
    "            self.variances[str(x)] = variances[()][str(x)]\n",
    "\n",
    "    # claulating the posterior using Bayes rule\n",
    "    def posterior2(self):\n",
    "        likelihoods = np.zeros((self.noData, self.classes))\n",
    "        for x in range(self.classes):\n",
    "            likelihoodClassC = self.logLikelihood(self.means[str(x)], self.variances[str(x)])\n",
    "            likelihoods[:, x] = likelihoodClassC + np.log(self.priors[str(x)])\n",
    "        self.likelyClass = np.argmax(likelihoods, axis=1)\n",
    "\n",
    "    # estimating the log likelihood for the data using multivariate gaussian density function\n",
    "    def logLikelihood(self, mu, sigma):\n",
    "        t1 = (-0.5 * np.sum(np.log(np.sqrt(sigma + self.eta)))) - (self.features * 0.5 * np.log(2 * math.pi))\n",
    "        diff = np.zeros(self.X.shape)\n",
    "        diff = (self.X - mu)\n",
    "        t2 = -1 * np.sum(np.power(diff, 2)/(0.5 * (sigma + self.eta)), axis=1)\n",
    "        return t1 + t2 #Return sum of first and second terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a8ba1",
   "metadata": {},
   "source": [
    "Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4aefcf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(idfValue):\n",
    "    \n",
    "    #preprocess the training data\n",
    "    preprocessObj = ForPreprocessing('datasets/spam_ham_dataset.csv', 1) #Absolute pathname. Make sure training dataset is present in same folder\n",
    "    preprocessObj.removeDup_makeLower()\n",
    "    preprocessObj.removeSpCharStopword()\n",
    "    preprocessObj.inflection(0)    \n",
    "    preprocessObj.saveProcessedTrainDataset('trainedEmails') \n",
    "    \n",
    "    #make the vocabulary for all the words in the organized training dataset\n",
    "    vocab = BuildVocab('trainedEmails.csv')\n",
    "    vocab.buildWordDict('wordsDict')\n",
    "    \n",
    "    #Extract features from data\n",
    "    dataPath = 'trainedEmails.csv' # 'test_emails.csv'\n",
    "    wordsDataPath = 'wordsDict.txt'\n",
    "    vec = Vectorize(dataPath, wordsDataPath, 1)\n",
    "    vec.vectorize(idfValue)\n",
    "    vec.saveData()\n",
    "    \n",
    "    #Output train accuracy of Naive bayes\n",
    "    X = np.load('Xtrain.npy')\n",
    "    Y = np.load('Ytrain.npy')\n",
    "    bayes = NBC(X , 1, datay=Y)    \n",
    "    bayes.findParameters()\n",
    "    bayes.posterior2()\n",
    "    print(\"Training data Accuracy: \")\n",
    "    print(sum(bayes.likelyClass == Y)/X.shape[0])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03f8ed",
   "metadata": {},
   "source": [
    "Testing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c90ec19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(idfValue = 0):\n",
    "\n",
    "    training(idfValue)\n",
    "    \n",
    "    #build the test dataset\n",
    "    testSet = TestSet('test')\n",
    "    testSet.makeTestCsv()\n",
    "    testSet.saveAsCsv()\n",
    "    testSet.cleanAndSave()\n",
    "\n",
    "    #extract features from the data\n",
    "    dataPath = 'testEmails.csv'\n",
    "    wordsDataPath = 'wordsDict.txt'\n",
    "    vec = Vectorize(dataPath, wordsDataPath, 0)\n",
    "    vec.vectorize(idfValue)\n",
    "    vec.saveData()\n",
    "\n",
    "    #run the naive bayes algorithm\n",
    "    X = np.load('Xtest.npy')\n",
    "    bayes = NBC(X, 0)\n",
    "    bayes.loadParameters()\n",
    "    bayes.posterior2()\n",
    "    print('Predictions for the emails in test folder : ')\n",
    "    print(bayes.likelyClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5e7f1",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "201b7a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Accuracy: \n",
      "0.9891761876127481\n",
      "Predictions for the emails in test folder : \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "idfValue = 0 #0-without idf values 1-with idf value @argv\n",
    "if len(sys.argv) > 1:\n",
    "    #idfValue = int(sys.argv[1])\n",
    "    idfValue = 1\n",
    "    \n",
    "testing(idfValue) #Fn call to run training procedure\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae498761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de49007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44ed45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cf153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202238a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e43a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c512cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
